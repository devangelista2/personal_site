---
layout: post
title: Clustering with Linear Discriminant Analysis (LDA)
description: >
    Clustering introduction with Linear Discriminant Analysis (LDA)
sitemap: false
hide_last_modified: true
---

## Why PCA is not sufficient?
Since PCA is an unsupervised learning technique, the lack of informations due to the fact that it is not using the associated label reduce its ability on observing clusters. Consequently it can be defined Linear Discriminant Analysis (LDA), a supervised version of PCA, whose ability on generating clusters is way higher.

## Setup for LDA implementation
The main difference between PCA and LDA is the the latter center the dataset by shifting each column by the centroid of each class, instead of using a global centroid of the data. For simplicity, assume we have a dataset 

$$
    X = [x^1 x^2 \dots x^N] \in \mathbb{R}^{d \times N}
$$

and an associated label vector

$$
    Y = [y^1 y^2 \dots y^N]
$$ 

such that $$y^i$$ represents the correct class of $$ x^i $$. Assume that in our problem there are $$K$$ classes $$C_1, \dots, C_K$$ and define $$I_k = [i_1, \dots, i_{N_k}]$$ as the vector of the indices corresponding to the columns of $X$ associated with the class $$k$$ ($$N_k$$ is the number of elements lying in the class $k$). To simplify the discussion, assume that our dataset is ordered as 

$$
    X = [X_1 X_2 \dots X_K]
$$

where $$X_k$$ contains each datapoint such that the corresponding class is $$k$$. Now, for each class $$k = 1, \dots, K$$ define the class centroid

$$
    c_k(X) = \frac{1}{N_k} \sum_{i \in I_k} x^i
$$

and the global centroid

$$
    c(X) = \frac{1}{N} \sum_{i=1}^N x^i
$$

For any $$k = 1, \dots, K$$, define the centered matrix for each class

$$
    X_{k, c} = X_k - c_k(X_k)
$$

and, by joining $$X_{k, c}$$ together for any $$k$$, we get

$$
    X_w = [X_{1, c} X_{2, c} \dots X_{K, c}]
$$

From $$X_w$$, we can define the **within-cluster scatter matrix** 

$$
    S_w = X_w X_w^T
$$

representing the _correlation matrix_ for points inside of each cluster.

Consequently, define

$$
\bar{X}_k = [c_k c_k c_k \dots c_k] \in \mathbb{R}^{d \times N} 
$$ 

$$
\bar{X} = [\bar{X}_1 \bar{X}_2 \dots \bar{X}_K]
$$

and its centered version

$$
\bar{X}_c = \bar{X} - c(X)
$$

Finally, we define the **between clusters scattering matrix**

$$
S_b = \bar{X}_c \bar{X}_c^T
$$

This concludes the setup part.

## Idea and Implementation
Given the two matrices $$S_w$$ and $$S_b$$ defined above, we observe that $$S_w$$ is symmetrical, since $$S_w^T = (X_w X_w^T)^T = X_w X_w^T = S_w$$. If $$S_w$$ is also positive definite, you studied that we can compute the Cholesky decomposition of it, i.e. we can decompose

$$
    S_w = L^T L
$$

where $$L \in \mathbb{R}^{d \times d}$$ is a lower-triangular matrix.

If $$S_w$$ is not positive definite, we can make it positive definite by doing

$$
    S_w = S_w + \epsilon I
$$

where $\epsilon \approx 10^{-6}$ is a small value. A matrix like that will always be positive definite. 

Now, given a number $$k$$ (representing the dimension on which we want to project), compute the matrix $$W \in \mathbb{R}^{d \times k}$$, whose column are the first $k$ eigenvectors of $$L^{-T}S_bL^T$$.

Finally, if

$$
    Q = L^{-1} W
$$

then $$Q^T \in \mathbb{R}^{k \times d}$$ will be the projection matrix of LDA. 

## Implementation
The implementation simply follows what we just described.

* Given the input data $$X$$ and the labels $$Y$$;
* Compute $$S_w$$ and $$S_b$$ as described above;
* If possible, compute the Cholesky decomposition of $$S_w = L^T L$$, if not, compute the Cholesky decomposition of $$S_w + \epsilon I = L^T L$$;
* Find the first $$k$$ eigenvectors of $$L^{-T}S_bL^T$$ and collect them in a matrix $$W$$;
* Compute $$Q = L^{-1} W$$ and $$Q^T$$.
* Since $$Q^T$$ is the projection matrix, we can project $$X$$ into $$Z$$ as $$Z = Q^T X$$.